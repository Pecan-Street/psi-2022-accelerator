{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook submits the jobs to aws batch using imported generated Latin hypercube sampling inputs generated from another notebook.\n",
    "\n",
    "It does not do a good job of monitoring the batch jobs, so your best bet is to log into the aws console and monitor the jobs there. \n",
    "I'm working on it, but it's not quite there yet.\n",
    "\n",
    "There's a variable set to True or False towards the end that you'll want to start with as False and change to True when jobs are done.\n",
    "After that, it will download all of the outfile.lis.csv files and then create a single .tgz file out of all of them in the outputs directory.\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import matrices.LHS_1000  # make sure the size is correct!\n",
    "\n",
    "\n",
    "matrix = matrices.LHS_1000.matrix  # here too!\n",
    "print(len(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(matrix)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment name/number\n",
    "exp = 'soybeans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('batch')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_uri = \"s3://daycent-jobs/jobs/{}_{}/jobs\".format(exp, size)\n",
    "jobs_uri\n",
    "# s3://daycent-jobs/jobs/corn_1000/diffs/diff_0.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3://daycent-jobs/jobs/corn_1000/diffs/diff_0.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# submit the jobs to aws batch \n",
    "# really ought to use a threadpool for submitting these (TODO)\n",
    "\n",
    "for i in range(size):  # size\n",
    "    job_name = 'test_{}_{}_{}'.format(exp, size, i)\n",
    "    response = client.submit_job(\n",
    "        jobDefinition='test-job-def',\n",
    "        jobName=job_name,\n",
    "        jobQueue='queue',\n",
    "        containerOverrides={\n",
    "          'command': [ \n",
    "              \"-s\",\"soybeans\",\n",
    "              \"-n\",\"soybeans\",\n",
    "              \"-e\",\"soybeans_prev\",\n",
    "              \"-f\",\"s3://daycent-jobs/jobs/soybeans/soybeans_prev.bin\",  # bin to extend from\n",
    "              \"-l\",\"yes\",  # run list100 after\n",
    "              \"-i\",\"s3://daycent-jobs/jobs/soybeans/soybeans.tgz\",  # full input directory\n",
    "              \"-j\", jobs_uri,  # job s3 bucket\n",
    "              # \"-o\",\"yes\",  # if you want to download the complete output directory rather than just the output file\n",
    "              \"-d\",\"s3://daycent-jobs/jobs/{}_{}/diffs/diff_{}.tgz\".format(exp, size, i),  # make sure this makes sense\n",
    "              \"-r\",\"{}\".format(i),\n",
    "              #\"-c\", \"yes\" # capture stderr and stdout to daycent.log.txt and list100.log.txt\n",
    "          ],  \n",
    "        },\n",
    "    )\n",
    "    jobs.append(response)\n",
    "    # print(response)\n",
    "print(\"Submitted {} jobs.\".format(len(jobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num jobs is {}\".format(len(jobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_listing = client.list_jobs(\n",
    "    jobQueue = 'queue',\n",
    "    jobStatus='RUNNABLE',\n",
    ")\n",
    "job_listing\n",
    "#print(\"Job list gave {} jobs\".format(len(job_listing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for job in jobs:\n",
    "    ids.append(job['jobId'])\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobstuff = client.describe_jobs(\n",
    "    jobs=ids[-100:]\n",
    ")\n",
    "for job in jobstuff['jobs']:\n",
    "    print(\"name: {}, status: {}\".format(job['jobName'], job['status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly samples a subset of jobs and prints their status to the output every few seconds for a few iterations\n",
    "# clears the previous output before printing the new one\n",
    "# you can just run this cell over and over again to get an idea of the jobs, not perfect, but it's somewhere to start\n",
    "\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "iterations = 40  # number of times to pull jobs and get their status\n",
    "num_jobs = 75 # number of jobs to randomly select from to view\n",
    "sleep_time = 5  # number of seconds to sleep between job polling\n",
    "\n",
    "for i in range(iterations):\n",
    "    done = True\n",
    "    jobstuff = client.describe_jobs(\n",
    "        jobs=list(random.sample(ids, num_jobs))\n",
    "    )\n",
    "    # jobstuff\n",
    "    clear_output(wait=True)\n",
    "    output = \"\"\n",
    "    for stuff in jobstuff['jobs']:\n",
    "        output = output + \"i={} : {} : {}\\n\".format(i, stuff['jobName'], stuff['status'])\n",
    "    print(output, end='\\r')\n",
    "    time.sleep(sleep_time)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "if not os.path.isdir('outputs/{}_{}'.format(exp, size)):\n",
    "    os.makedirs('outputs/{}_{}'.format(exp, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip this to True when the jobs are finished running, or put in False to pause here while they're still running\n",
    "jobs_done = True\n",
    "jobs_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# copy all the outfile.lis.csv files back to the outputs directory named to correspond to their job number\n",
    "# really ought to use a threadpool for submitting these (TODO)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# just holding things up here to wait for the jobs to finish since I can't check them all because aws doesn't like checking on more than\n",
    "# 100 jobs at a time. Change to jobs_done True to continue\n",
    "if jobs_done: \n",
    "    \n",
    "    for i in range(len(matrix)):\n",
    "        # grab the csv files and stick them somewhere\n",
    "        # cmd should look like:\n",
    "        # aws s3 cp s3://daycent-jobs/jobs/exp1000/jobs/{}/outfile.lis.csv ./outputs/exp1000/outfile_{}.lis.csv\n",
    "        subprocess.call(['/home/jovyan/aws-cli/v2/current/bin/aws', 's3', 'cp', \n",
    "                         '{}/{}/outfile.lis.csv'.format(jobs_uri, i), \n",
    "                         './outputs/{}_{}/outfile_{}.list.csv'.format(exp, size, i)])\n",
    "print(\"sure done, why not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create a single tgz file containing all of the outfile_{job}.lis.csv files\n",
    "\n",
    "# just holding things up here to wait for the jobs to finish since I can't check them all because aws doesn't like checking on more than\n",
    "# 100 jobs at a time. Change jobs_done to True to continue\n",
    "if jobs_done:\n",
    "    import os\n",
    "\n",
    "    os.chdir('outputs')\n",
    "    print(os.getcwd())\n",
    "\n",
    "    # tgz them all up\n",
    "    subprocess.call(['tar', 'czvf', '{}_{}.tgz'.format(exp, size), '{}_{}'.format(exp, size)])\n",
    "    print('done tgzing them')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
