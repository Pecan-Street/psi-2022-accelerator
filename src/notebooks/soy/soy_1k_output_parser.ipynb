{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook takes the large *.tgz file created from the batch jobs notebook, as well as the generated inputs from the MatrixBuilder notebook\n",
    "and creates tensor files to be imported to train and run the model. \n",
    "\n",
    "You should make sure you're importing the correct size matrix from the matrices directory, and set your exp (experiment name) to match the one from your batch jobs notebook.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# imports the pre-generated N input matrix into a python list\n",
    "# make sure this is the correct size for the number of jobs you want to run!\n",
    "import matrices.LHS_1000\n",
    "\n",
    "# grab the input matrix, again make sure this is correct!!\n",
    "matrix = matrices.LHS_1000.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_size = len(matrix)\n",
    "exp = 'soybeans'  # Be sure to set this correctly for your generated outputs from the batch job notebook!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how we want to split up the inputs and outputs. Currently set to 80%, 10%, 10%\n",
    "splits = [int(matrix_size * .8), int(matrix_size * .9)]\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where's our tgz file from the previous notebook?\n",
    "gz_file = 'outputs/{}_{}.tgz'.format(exp, matrix_size)\n",
    "gz_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(gz_file, \"r:*\")\n",
    "# tar.getmembers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# read in all of the csv files into separate pandas dataframes and store those in a list\n",
    "# this can take ~40 minutes when there's 10k of them\n",
    "\n",
    "csv_files = tar.getmembers()[1:]  # skips the first one because it's a directory not a file\n",
    "csv_files = sorted(csv_files, key=lambda m: m.name)  # they come out in a very odd order, want them 0-9 sorted\n",
    "print(len(csv_files))\n",
    "dfs = []\n",
    "for csv in csv_files:\n",
    "    df = pd.read_csv(tar.extractfile(csv))\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# convert list to numpy array of type float32\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "np_inputs = np.array(matrix, dtype=\"float32\")\n",
    "np_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# split the numpy input array into 3 chunks (training, test, validate)\n",
    "[in_train_np, in_validate_np, in_test_np] = np.array_split(np_inputs, splits)\n",
    "print('train: {}, val: {}, test: {}'.format(len(in_train_np), len(in_validate_np), len(in_test_np)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# grab just the columns we want. And right now we're just grabbing the last value of each of them\n",
    "\n",
    "np_outs_list = []\n",
    "for df in dfs:\n",
    "    np_outs_list.append([df['somtc'].iat[-1], df['somsc'].iat[-1], df['agcprd'].iat[-1], \n",
    "                         df['cgrain'].iat[-1], df['stemp'].iat[-1]])\n",
    "    # print('appending somsc={} and bglivcj={}'.format(df['somsc'].iat[-1], df['bglivcj'].iat[-1]))\n",
    "len(np_outs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np_outs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create a Numpy array\n",
    "\n",
    "np_outs = np.array(np_outs_list, dtype=\"float32\")\n",
    "np_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# make the input tensors\n",
    "in_train_tensor = torch.from_numpy(in_train_np)\n",
    "print(in_train_tensor)\n",
    "in_validate_tensor = torch.from_numpy(in_validate_np)\n",
    "print(in_train_tensor)\n",
    "in_test_tensor = torch.from_numpy(in_test_np)\n",
    "print(in_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the input tensors to disk\n",
    "np.save('outputs/{}_{}_in_train_tensor.npy'.format(exp, matrix_size), in_train_tensor)\n",
    "np.save('outputs/{}_{}_in_test_tensor.npy'.format(exp, matrix_size), in_test_tensor)\n",
    "np.save('outputs/{}_{}_in_validate_tensor.npy'.format(exp, matrix_size), in_validate_tensor)\n",
    "print('done saving input tensor files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the outputs in to the 80-10-10\n",
    "[out_train_np, out_validate_np, out_test_np] = np.array_split(np_outs, splits)\n",
    "print('train: {}, val: {}, test: {}'.format(len(out_train_np), len(out_validate_np), len(out_test_np)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# make the output tensors\n",
    "out_train_tensor = torch.from_numpy(out_train_np)\n",
    "print(out_train_tensor)\n",
    "out_validate_tensor = torch.from_numpy(out_validate_np)\n",
    "print(out_train_tensor)\n",
    "out_test_tensor = torch.from_numpy(out_test_np)\n",
    "print(out_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the output tensors to disk\n",
    "np.save('outputs/{}_{}_out_train_tensor.npy'.format(exp, matrix_size), out_train_tensor)\n",
    "np.save('outputs/{}_{}_out_test_tensor.npy'.format(exp, matrix_size), out_test_tensor)\n",
    "np.save('outputs/{}_{}_out_validate_tensor.npy'.format(exp, matrix_size), out_validate_tensor)\n",
    "print('done saving output tensor files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the tensors\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of in_train_tensor: {in_train_tensor.shape}\")\n",
    "print(f\"Datatype of in_train_tensor: {in_train_tensor.dtype}\")\n",
    "print(f\"Device in_train_tensor is stored on: {in_train_tensor.device}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of in_test_tensor : {in_test_tensor.shape}\")\n",
    "print(f\"Datatype of in_test_tensor: {in_test_tensor.dtype}\")\n",
    "print(f\"Device in_test_tensor is stored on: {in_test_tensor.device}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of in_validate_tensor: {in_validate_tensor.shape}\")\n",
    "print(f\"Datatype of in_validate_tensor: {in_validate_tensor.dtype}\")\n",
    "print(f\"Device in_validate_tensor is stored on: {in_validate_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the tensors\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of out_train_tensor: {out_train_tensor.shape}\")\n",
    "print(f\"Datatype of out_train_tensor: {out_train_tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {out_train_tensor.device}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of out_test_tensor : {out_test_tensor.shape}\")\n",
    "print(f\"Datatype of out_test_tensor: {out_test_tensor.dtype}\")\n",
    "print(f\"Device out_test_tensor is stored on: {out_test_tensor.device}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of out_validate_tensor: {out_validate_tensor.shape}\")\n",
    "print(f\"Datatype of out_validate_tensor: {out_validate_tensor.dtype}\")\n",
    "print(f\"Device out_validate_tensor is stored on: {out_validate_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to reload them from disk kinda looks like this, but I'm not going to do that.\n",
    "\n",
    "out_np = np.load('outputs/1k_outputs.npy')\n",
    "out_tensor = torch.from_numpy(out_np)\n",
    "out_tensor\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
